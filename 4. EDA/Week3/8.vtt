WEBVTT

1
00:00:00.840 --> 00:00:02.870
SVD 和主成分分析 都存在着一个问题

2
00:00:02.870 --> 00:00:05.850
就是无法应用于有缺失值的数据

3
00:00:05.850 --> 00:00:08.460
但实际的数据通常都含有缺失值

4
00:00:08.460 --> 00:00:10.490
如果你试着

5
00:00:10.490 --> 00:00:12.700
对这些有缺失值的数据集做 SVD 的话

6
00:00:12.700 --> 00:00:14.870
就比如 对我创建的这个数据集做 SVD

7
00:00:14.870 --> 00:00:16.860
可以看到 返回的是错误信息

8
00:00:16.860 --> 00:00:19.510
你无法对有缺失值的数据集使用 SVD

9
00:00:19.510 --> 00:00:22.470
所以 在做 SVD 或者 PCA 之前

10
00:00:22.470 --> 00:00:24.560
我们需要对缺失值进行一些处理

11
00:00:26.160 --> 00:00:29.640
可用的方法有很多种 其中一种是

12
00:00:29.640 --> 00:00:33.740
我们可以利用 impute 包 这个包可以从 bioconductor 的网站下载到

13
00:00:33.740 --> 00:00:38.330
利用 impute 我们可以在所有有缺失值的地方

14
00:00:38.330 --> 00:00:40.740
都补上一个值 然后我们就可以运行 SVD 了

15
00:00:40.740 --> 00:00:42.210
这里我们用到的是 impute.knn 函数

16
00:00:42.210 --> 00:00:47.920
这里我们用到的是 impute.knn 函数

17
00:00:47.920 --> 00:00:51.460
它将会利用最靠近缺失行或缺失值的 k 行里的值

18
00:00:51.460 --> 00:00:52.790
来填充缺失的值

19
00:00:52.790 --> 00:00:56.050
例如 如果 k 等于5的话

20
00:00:56.050 --> 00:00:58.790
它会选择距离含有缺失值的行最近的5行数据

21
00:00:58.790 --> 00:01:01.290
并用这5行的某种均值

22
00:01:01.290 --> 00:01:04.220
来填充缺失的数据

23
00:01:04.220 --> 00:01:06.780
一旦我们利用 impute.knn 函数对数据进行填充之后

24
00:01:06.780 --> 00:01:10.140
就能对它进行 SVD 了

25
00:01:10.140 --> 00:01:11.400
可以看到 这次没有出现错误信息

26
00:01:11.400 --> 00:01:16.600
让我们来画出

27
00:01:16.600 --> 00:01:19.430
第一个奇异向量的图像

28
00:01:19.430 --> 00:01:23.390
左侧的图所表示的 是我们从原数据矩阵所得到的

29
00:01:23.390 --> 00:01:25.040
左侧的图所表示的 是我们从原数据矩阵所得到的

30
00:01:25.040 --> 00:01:28.010
第一个奇异向量的值

31
00:01:28.010 --> 00:01:29.570
右侧的图

32
00:01:29.570 --> 00:01:31.320
则是我们填充了第二个有缺失值的矩阵之后

33
00:01:31.320 --> 00:01:35.410
所得到的第一个奇异向量的值

34
00:01:35.410 --> 00:01:37.030
两张图可以说是非常相似的

35
00:01:37.030 --> 00:01:39.170
虽然并不完全相同

36
00:01:39.170 --> 00:01:42.150
但我们的填充值并没有

37
00:01:42.150 --> 00:01:44.610
对 SVD 的结果造成很大的影响

38
00:01:46.840 --> 00:01:49.928
接下来我们来看最后一个例子 这个例子很有意思

39
00:01:49.928 --> 00:01:51.150
我想在这里示范一下

40
00:01:51.150 --> 00:01:54.197
如果我们有一张图片 被写成了矩阵的形式

41
00:01:54.197 --> 00:01:57.460
我们怎么样才能

42
00:01:57.460 --> 00:02:01.030
把这张图片用更低的维度或者更小的秩表现出来

43
00:02:01.030 --> 00:02:02.480
这个图片里是一张脸

44
00:02:02.480 --> 00:02:05.480
当然 图的解析度相对比较低了

45
00:02:05.480 --> 00:02:08.440
但我们还是能看清他的鼻子、耳朵、两只眼睛还有嘴巴的

46
00:02:08.440 --> 00:02:11.940
我们的目标是 对这张脸的数据

47
00:02:11.940 --> 00:02:15.000
来做一次 SVD 看看方差解释值的情况

48
00:02:15.000 --> 00:02:17.150
从结果中可以看到 

49
00:02:17.150 --> 00:02:20.570
第一个奇异值解释了大约40%的方差

50
00:02:20.570 --> 00:02:23.100
第二个则解释了20%多一些

51
00:02:23.100 --> 00:02:25.796
第三个大约是15%吧

52
00:02:25.796 --> 00:02:30.870
可以看出 前5个 或者前10个奇异值

53
00:02:30.870 --> 00:02:35.070
已经捕捉到了数据集几乎所有的变化

54
00:02:35.070 --> 00:02:37.350
因此 我们可以试着来看看 

55
00:02:37.350 --> 00:02:40.100
只用第1个 或者只用前5个 前10个的奇异值 

56
00:02:40.100 --> 00:02:42.940
所生成的图片的样子

57
00:02:44.580 --> 00:02:47.410
要实现这个想法

58
00:02:47.410 --> 00:02:49.200
我们需要用到一些矩阵乘法

59
00:02:49.200 --> 00:02:52.840
来创建一个与这张脸近似的图片

60
00:02:53.890 --> 00:02:58.250
但新图片矩阵的成分要少于源数据集

61
00:02:58.250 --> 00:02:59.790
这里我们先来创建第一个图

62
00:02:59.790 --> 00:03:03.380
这张图只用到了第一个主成分和第一个奇异向量

63
00:03:03.380 --> 00:03:06.140
接下来这个则用前5个

64
00:03:06.140 --> 00:03:08.178
最后再用前10个

65
00:03:08.178 --> 00:03:12.000
我们来看看我们得到的近似图片是什么样的

66
00:03:12.000 --> 00:03:13.760
最左边这一张图

67
00:03:13.760 --> 00:03:16.960
我们只用了一个奇异向量

68
00:03:16.960 --> 00:03:20.580
可以看出结果并不是太好 并没有构成一张图片的样子

69
00:03:20.580 --> 00:03:21.630
图中的脸也完全看不出来

70
00:03:21.630 --> 00:03:23.050
什么也看不出来

71
00:03:23.050 --> 00:03:29.610
只用一个奇异值来表达出一整张图 确实不太现实

72
00:03:29.610 --> 00:03:32.010
那么 我们来看看左数第二张图

73
00:03:32.010 --> 00:03:36.840
这张图已经基本能看出大部分关键的特征了

74
00:03:36.840 --> 00:03:39.190
这里用到了前5个奇异向量

75
00:03:39.190 --> 00:03:41.130
我们可以清楚地看出这是一张脸

76
00:03:41.130 --> 00:03:43.250
两只眼睛、一个鼻子、一张嘴还有两只耳朵

77
00:03:44.580 --> 00:03:46.590
和原图并不完全相同

78
00:03:46.590 --> 00:03:48.910
但已经非常接近了

79
00:03:49.950 --> 00:03:52.540
我们再来看看下一张图 也就是图 c

80
00:03:52.540 --> 00:03:54.570
这张图相对来说

81
00:03:54.570 --> 00:03:56.150
就更加清晰了一点

82
00:03:56.150 --> 00:03:59.120
这里我们用到了前10个奇异向量

83
00:03:59.120 --> 00:04:04.300
但它和第二张图的区别并不太大 而第二张只用了5个奇异向量而已

84
00:04:04.300 --> 00:04:07.000
最右边的这张图 是我们的原始数据生成的

85
00:04:07.000 --> 00:04:09.130
从这些图里我们可以看出

86
00:04:09.130 --> 00:04:12.080
只用一个 或者最多5个10个的奇异向量

87
00:04:12.080 --> 00:04:16.040
我们就可以得到一个非常不错的近似图

88
00:04:16.040 --> 00:04:19.050
这样 我们就不用存储所有的原始数据了

89
00:04:19.050 --> 00:04:19.970
这就是利用奇异值分解

90
00:04:19.970 --> 00:04:22.954
这就是利用奇异值分解

91
00:04:22.954 --> 00:04:27.520
来进行数据压缩的一个例子

92
00:04:27.520 --> 00:04:30.320
现今 数据压缩和统计汇总

93
00:04:30.320 --> 00:04:32.430
就像是一枚硬币的正反两面

94
00:04:32.430 --> 00:04:35.480
如果你想要用尽量少的特征

95
00:04:35.480 --> 00:04:37.860
对一个数据集进行总结的话

96
00:04:37.860 --> 00:04:40.700
奇异值分解就是一个很好的方法

97
00:04:44.150 --> 00:04:46.170
最后 补充一些关于

98
00:04:46.170 --> 00:04:49.710
奇异值分解和主成分分析的注意点和资源

99
00:04:49.710 --> 00:04:52.740
首先 数据的尺度是很重要的

100
00:04:52.740 --> 00:04:54.860
比如 通常我们的数据中

101
00:04:54.860 --> 00:04:58.700
很多不同的变量常常拥有非常不同的尺度

102
00:04:58.700 --> 00:05:01.300
这会对结果造成一些问题

103
00:05:01.300 --> 00:05:04.540
如果只是因为测量单位不同 

104
00:05:04.540 --> 00:05:07.070
造成其中一个变量远大于其它的变量的话

105
00:05:07.070 --> 00:05:10.080
主成分分析和奇异值分解的结果都会受到影响

106
00:05:10.080 --> 00:05:13.140
这样的结果可能就没有什么意义了

107
00:05:13.140 --> 00:05:16.100
所以 你必须先检查一下数据

108
00:05:16.100 --> 00:05:19.740
保证不同列 不同行的值 都大概在一个可比较的尺度上

109
00:05:21.920 --> 00:05:24.210
还有就是 正如我们在之前的一个例子里看到的

110
00:05:24.210 --> 00:05:26.020
如果数据同时有两种不同模式的话 

111
00:05:26.020 --> 00:05:29.170
主成分和奇异向量可能会把这些模式都混在一起

112
00:05:29.170 --> 00:05:32.070
所以你看到的结果

113
00:05:32.070 --> 00:05:33.350
可能并不是清楚的两种单独的模式

114
00:05:33.350 --> 00:05:36.020
而是它们混合后的结果

115
00:05:36.020 --> 00:05:39.600
如果矩阵非常大的话 奇异值分解所要消耗的计算量

116
00:05:39.600 --> 00:05:42.890
也会非常的大 这点在应用中也需要考虑到

117
00:05:42.890 --> 00:05:45.000
我们一般用的都是相对小的矩阵

118
00:05:45.000 --> 00:05:46.930
当然 现在计算机的计算速度越来越快了

119
00:05:48.210 --> 00:05:51.880
也有一些算法高度优化

120
00:05:51.880 --> 00:05:53.680
专门用于矩阵计算的程序库

121
00:05:53.680 --> 00:05:55.940
可以更快地进行奇异值分解

122
00:05:55.940 --> 00:05:58.810
这个方法也不需要太多的前期准备

123
00:06:00.150 --> 00:06:03.720
就可以直接应用于实际问题当中了

124
00:06:04.790 --> 00:06:07.780
这里还有一些有用的链接

125
00:06:07.780 --> 00:06:09.310
都是关于主成分分析

126
00:06:09.310 --> 00:06:11.590
和奇异值分解的资源

127
00:06:11.590 --> 00:06:13.370
当然还有一些与它们类似的方法

128
00:06:13.370 --> 00:06:15.860
当然还有一些与它们类似的方法

129
00:06:15.860 --> 00:06:19.560
但这些方法在细节上可能有很大的不同

130
00:06:19.560 --> 00:06:20.770
你可能已经听说过其中一些

131
00:06:20.770 --> 00:06:22.480
比如因子分析 (factor analysis)

132
00:06:22.480 --> 00:06:25.400
独立成分分析 (independent component analysis) 和潜在语义分析 (latent semantic analysis)

133
00:06:25.400 --> 00:06:28.940
这些方法也是很值得探究的

134
00:06:28.940 --> 00:06:31.070
但它们背后的基本思想

135
00:06:31.070 --> 00:06:33.620
都和主成分分析以及奇异值分解互相关联

136
00:06:33.620 --> 00:06:34.790
这个基本思想就是

137
00:06:34.790 --> 00:06:39.180
我们想要用更低的维度

138
00:06:39.180 --> 00:06:40.250
最大程度地来解释数据的变化
【教育无边界字幕组】ChelseaT | hazard1990 | HikaruSama