WEBVTT

1
00:00:00.460 --> 00:00:03.118
这节课要讲的是 K 均值聚类 (K-means clustering)

2
00:00:03.118 --> 00:00:06.260
K 均值实际上是一个相对久远的技术 很久之前就出现了

3
00:00:06.260 --> 00:00:09.820
但是它一直非常有用

4
00:00:09.820 --> 00:00:11.910
我们用它总结高维数据 (high dimensional data)

5
00:00:11.910 --> 00:00:15.220
并且可以大致了解数据展现出的规律

6
00:00:15.220 --> 00:00:17.000
什么样的观测值会比较相似

7
00:00:17.000 --> 00:00:19.440
什么样的观测值互不相同

8
00:00:19.440 --> 00:00:21.120
我们会稍微讲一讲 K 均值是怎么使用的

9
00:00:21.120 --> 00:00:23.120
看看它能够对大家有什么帮助

10
00:00:24.360 --> 00:00:25.655
K 均值聚类背后的基本原则就是

11
00:00:25.655 --> 00:00:31.050
K 均值聚类背后的基本原则就是

12
00:00:31.050 --> 00:00:32.300
第一 它定义了什么是相似

13
00:00:32.300 --> 00:00:33.830
什么是不同

14
00:00:33.830 --> 00:00:35.880
也就是说 你想要定义

15
00:00:35.880 --> 00:00:37.470
什么叫做相近?

16
00:00:37.470 --> 00:00:41.340
我们怎样来进行分组 以及 我们如何把这样的分组可视化

17
00:00:41.340 --> 00:00:45.410
一旦完成可视化之后 我们如何来诠释我们所看到的?

18
00:00:45.410 --> 00:00:46.740
我认为最重要的就是下定义 如何定义相近?

19
00:00:46.740 --> 00:00:49.310
我认为最重要的就是下定义 如何定义相近?

20
00:00:49.310 --> 00:00:50.700
我们需要一个表示距离的度量 (metric)

21
00:00:50.700 --> 00:00:54.820
来定义什么才算两者相近似

22
00:00:54.820 --> 00:00:57.030
因为根据情景不同

23
00:00:57.030 --> 00:00:59.010
两者可能看起来相近 但是其实没有那么近

24
00:01:00.400 --> 00:01:01.840
在不同的情境下

25
00:01:01.840 --> 00:01:04.590
你可能会对距离有完全不同的解释

26
00:01:04.590 --> 00:01:06.080
因此这是一个非常重要的步骤

27
00:01:06.080 --> 00:01:09.040
如果你不把它做好 到最后你会发现这些东西全无意义

28
00:01:09.040 --> 00:01:11.030
也就是很经典的“烂进烂出” (garbage in, garbage out) 情况

29
00:01:12.080 --> 00:01:16.040
因在统计中一些传统的距离度量里面

30
00:01:16.040 --> 00:01:19.070
有一个连续距离的概念

31
00:01:19.070 --> 00:01:20.560
比如说 "欧氏距离" (euclidean distance)

32
00:01:20.560 --> 00:01:22.280
也就是两点间的直线

33
00:01:23.440 --> 00:01:27.230
另一种连续测量值是 两个变量的近似相关 (correlation similarity)

34
00:01:27.230 --> 00:01:29.410
另一种连续测量值是 两个变量的近似相关 (correlation similarity)

35
00:01:29.410 --> 00:01:31.160
这样你可以知道它们有多少相关 或者说它们有多相似

36
00:01:31.160 --> 00:01:33.600
相关度高的两个点就会相似

37
00:01:34.950 --> 00:01:38.440
还有一种连续距离叫做曼哈顿距离 (Manhattan distance)

38
00:01:38.440 --> 00:01:39.470
关于这个内容我们之后会再介绍

39
00:01:39.470 --> 00:01:42.200
关于这个内容我们之后会再介绍

40
00:01:42.200 --> 00:01:44.030
所以说 你需要选择一个距离 或者是一个相似度的度量标准

41
00:01:44.030 --> 00:01:46.570
来让你的问题变得有意义

42
00:01:49.052 --> 00:01:53.010
K 均值聚类是一种将一组观测值划分成一定数量簇 (cluster) 的方法

43
00:01:53.010 --> 00:01:56.690
K 均值聚类是一种将一组观测值划分成一定数量簇 (cluster) 的方法

44
00:01:56.690 --> 00:01:58.570
你必须在开始的时候确定簇的数量

45
00:01:58.570 --> 00:02:01.880
你必须在开始的时候确定簇的数量

46
00:02:01.880 --> 00:02:05.620
比如说 在你的数据集中有 100 个观测值

47
00:02:05.620 --> 00:02:09.520
你可能想把它们分在四个不同的组里

48
00:02:09.520 --> 00:02:09.830
你可能想把它们分在四个不同的组里

49
00:02:09.830 --> 00:02:13.340
因此从一开始你就知道有多少个簇

50
00:02:13.340 --> 00:02:14.180
接着每个组都会有一个形心 (centroid)

51
00:02:14.180 --> 00:02:15.770
接着每个组都会有一个几何中心 (centroid)

52
00:02:15.770 --> 00:02:16.920
它就像是每一组围聚起来的一个重心

53
00:02:16.920 --> 00:02:20.780
它就像是每一组围聚起来的一个重心

54
00:02:22.020 --> 00:02:25.540
一旦有了这些几何中心

55
00:02:25.540 --> 00:02:28.410
我们可以把每一个观测值分配给一个几何中心

56
00:02:28.410 --> 00:02:31.300
这就是布置分组的方法

57
00:02:31.300 --> 00:02:33.790
因此 编写 K 均值算法的基本方法就是选择一个或一些几何中心

58
00:02:33.790 --> 00:02:37.540
因此 编写 K 均值算法的基本方法就是选择一个或一些几何中心

59
00:02:37.540 --> 00:02:40.020
把所有点分配给这些几何中心

60
00:02:40.020 --> 00:02:42.320
然后重新计算这些几何中心 再重新分配所有点

61
00:02:42.320 --> 00:02:45.422
就这样反复操作直到得到答案

62
00:02:45.422 --> 00:02:49.230
因此 你需要一个距离度量

63
00:02:49.230 --> 00:02:50.610
需要一些簇

64
00:02:50.610 --> 00:02:53.830
预先划分出的一定数量的簇

65
00:02:53.830 --> 00:02:57.210
并且需要对几何中心的位置有一个初始的猜测

66
00:02:57.210 --> 00:02:59.920
通常你可能随机选择一些点作为几何中心的位置

67
00:02:59.920 --> 00:03:02.576
这样做只是为了让算法开始运行

68
00:03:02.576 --> 00:03:05.230
但是 K 均值聚类算法会得到

69
00:03:05.230 --> 00:03:10.156
一个对于几何中心位置的最终估计

70
00:03:10.156 --> 00:03:15.400
它会告诉你每个观测值分配到哪一个几何中心

71
00:03:15.400 --> 00:03:19.590
这里有一个简单的例子 告诉你如何使用 K 均值聚类算法

72
00:03:19.590 --> 00:03:22.480
我这里已经生成了一些随机数据

73
00:03:22.480 --> 00:03:24.950
这些数据是两维的 这样便于可视化

74
00:03:24.950 --> 00:03:26.500
x 坐标和 y 坐标都来源于正态分布

75
00:03:26.500 --> 00:03:29.880
但是两个分布的平均值不同

76
00:03:29.880 --> 00:03:31.020
我在这里特别为12个观测值创建了3个簇

77
00:03:31.020 --> 00:03:36.250
我在这里特别为12个观测值创建了3个簇

78
00:03:36.250 --> 00:03:38.930
因此每个簇里有4个观测值

79
00:03:38.930 --> 00:03:43.918
当我绘制数据的时候 很明显这里有3个簇

80
00:03:43.918 --> 00:03:46.170
然后我给每个点加上标签

81
00:03:47.390 --> 00:03:49.350
在这里我们看到算法开始了

82
00:03:49.350 --> 00:03:52.000
我选了3个随机的点作为几何中心

83
00:03:52.000 --> 00:03:52.810
我选了3个随机的点作为几何中心

84
00:03:52.810 --> 00:03:55.990
你可以看到有一个红色加号 一个紫色加号

85
00:03:55.990 --> 00:03:57.520
以及在底部的一个橙色加号

86
00:03:57.520 --> 00:04:01.270
这3个就是几何中心的随机起始点

87
00:04:01.270 --> 00:04:03.871
我们要做的第一件事是

88
00:04:03.871 --> 00:04:05.791
读取每一个数据点

89
00:04:05.791 --> 00:04:08.760
并把它们分配给最近的几何中心

90
00:04:08.760 --> 00:04:11.610
所以你可以看到 比如最上方的8号点

91
00:04:11.610 --> 00:04:14.870
离它最近的几何中心是左上方的红色加号

92
00:04:14.870 --> 00:04:17.070
于是把它被分配给了这个几何中心

93
00:04:17.070 --> 00:04:18.620
然后左下方的4号点

94
00:04:18.620 --> 00:04:21.890
也是分配给红色加号

95
00:04:21.890 --> 00:04:24.330
然后是这三个点 1号、2号、3号

96
00:04:24.330 --> 00:04:27.570
它们都分在橙色加号的橙色簇里

97
00:04:27.570 --> 00:04:30.820
然后 剩下的点都是离紫色加号最近的

98
00:04:30.820 --> 00:04:32.770
因此 它们都分配给紫色加号

99
00:04:32.770 --> 00:04:35.660
这就是第一轮初步分组

100
00:04:35.660 --> 00:04:39.380
把这些点分给这3个不同的簇

101
00:04:40.520 --> 00:04:41.380
接下来我们要做的是

102
00:04:41.380 --> 00:04:42.930
重新计算几何中心

103
00:04:42.930 --> 00:04:44.660
现在我们把簇都确定下来了

104
00:04:44.660 --> 00:04:47.640
每个点都分配给了一个簇

105
00:04:47.640 --> 00:04:51.090
我们可以重新计算几何中心 比如说 通过计算这个簇的平均值

106
00:04:51.090 --> 00:04:53.500
现在你可以看到

107
00:04:53.500 --> 00:04:56.700
紫色加号向那个簇的中间稍微移动了一点

108
00:04:56.700 --> 00:04:59.200
红色加号也向那个簇的中间稍微移动了一点

109
00:04:59.200 --> 00:05:02.170
橙色加号现在在三个橙色点的中间了

110
00:05:02.170 --> 00:05:06.410
簇的几何中心被重新计算了

111
00:05:06.410 --> 00:05:08.040
接着 你再重复这个过程

112
00:05:08.040 --> 00:05:13.010
取出每个数据点

113
00:05:13.010 --> 00:05:14.940
看看现在哪个簇的几何中心离它最近

114
00:05:14.940 --> 00:05:17.530
比如 我可以看到4号点以前在红色簇

115
00:05:17.530 --> 00:05:21.110
但是现在它离橙色加号更近了 因此它被重新分配到橙色簇

116
00:05:21.110 --> 00:05:24.310
再比如 你看到7号点现在离红色加号更近

117
00:05:24.310 --> 00:05:27.100
因此它属于红色簇

118
00:05:27.100 --> 00:05:30.330
接下来我们重新计算几何中心的位置

119
00:05:30.330 --> 00:05:33.310
你看到现在几何中心又向这些点的中心移动了

120
00:05:33.310 --> 00:05:38.390
这样一步步走下去 你就知道簇是如何形成的了

121
00:05:38.390 --> 00:05:42.270
只有两个步骤 就是把点分配给每个簇

122
00:05:42.270 --> 00:05:45.880
然后更新几何中心的位置
【教育无边界字幕组】October Sky | 詹詹詹米 | hazard1990